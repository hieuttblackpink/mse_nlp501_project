{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2d27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_pos import get_word_tag, preprocess\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import w2_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e057b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "with open(\"./data/WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48e520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the vocabulary list\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "# read the vocabulary data, split by each line of text, and save the list\n",
    "with open(\"./data/hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "\n",
    "print(\"A few items of the vocabulary list\")\n",
    "print(voc_l[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voc_l[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1dcbc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      ":0\n",
      "!:1\n",
      "#:2\n",
      "$:3\n",
      "%:4\n",
      "&:5\n",
      "':6\n",
      "'':7\n",
      "'40s:8\n",
      "'60s:9\n",
      "'70s:10\n",
      "'80s:11\n",
      "'86:12\n",
      "'90s:13\n",
      "'N:14\n",
      "'S:15\n",
      "'d:16\n",
      "'em:17\n",
      "'ll:18\n",
      "'m:19\n",
      "'n':20\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {}\n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7bdf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the test corpus\n",
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the test corpus\n",
    "with open(\"./data/WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "    \n",
    "print(\"A sample of the test corpus\")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad50118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  34199\n",
      "This is a sample of the test_corpus: \n",
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"./data/test.words\")     \n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is a sample of the test_corpus: ')\n",
    "print(prep[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f679e",
   "metadata": {},
   "source": [
    "## Exercise 1 - create_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f1bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 GRADED FUNCTION: create_dictionaries\n",
    "def create_dictionaries(training_corpus, vocab, verbose=True):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the dictionaries using defaultdict\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # use 'i' to track the line number in the corpus\n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "        \n",
    "        # Every 50,000 words, print the word count\n",
    "        if i % 50000 == 0 and verbose:\n",
    "            print(f\"word count = {i}\")\n",
    "            \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
    "        # the function is defined as: get_word_tag(line, vocab)\n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd16b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n",
      "word count = 100000\n",
      "word count = 150000\n",
      "word count = 200000\n",
      "word count = 250000\n",
      "word count = 300000\n",
      "word count = 350000\n",
      "word count = 400000\n",
      "word count = 450000\n",
      "word count = 500000\n",
      "word count = 550000\n",
      "word count = 600000\n",
      "word count = 650000\n",
      "word count = 700000\n",
      "word count = 750000\n",
      "word count = 800000\n",
      "word count = 850000\n",
      "word count = 900000\n",
      "word count = 950000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "282f63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "View these POS tags (states)\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee73268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_create_dictionaries(create_dictionaries, training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c271db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n",
      "emission examples: \n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "\n",
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print (ex)\n",
    "print()\n",
    "\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8415ce",
   "metadata": {},
   "source": [
    "## Exercise 2 - predict_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f91ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 GRADED FUNCTION: predict_pos\n",
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    # Initialize the number of correct predictions to zero\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    # Initialize total count to 0 \n",
    "    total = 0\n",
    "    for word, y_tup in zip(prep, y): \n",
    "\n",
    "        # Split the (word, POS) string into a list of two items\n",
    "        y_tup_l = y_tup.split()\n",
    "        \n",
    "        # Verify that y_tup contain both word and POS\n",
    "        if len(y_tup_l) == 2:\n",
    "            \n",
    "            # Set the true POS label for this word\n",
    "            true_label = y_tup_l[1]\n",
    "\n",
    "        else:\n",
    "            # If the y_tup didn't contain word and POS, go to next word\n",
    "            continue\n",
    "    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        # If the word is in the vocabulary...\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "\n",
    "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            \n",
    "                # define the key as the tuple containing the POS and word\n",
    "                key = (pos, word)\n",
    "\n",
    "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
    "                if key in all_words: # Replace None in this line with the proper condition.\n",
    "\n",
    "                # get the emission count of the (pos,word) tuple \n",
    "                    count = emission_counts[key]\n",
    "\n",
    "                    # keep track of the POS with the largest count\n",
    "                    if count > count_final: # Replace None in this line with the proper condition.\n",
    "\n",
    "                        # update the final count (largest count)\n",
    "                        count_final = count\n",
    "\n",
    "                        # update the final POS\n",
    "                        pos_final = pos\n",
    "\n",
    "            # If the final POS (with the largest count) matches the true POS:\n",
    "            if pos_final == true_label: # Replace None in this line with the proper condition.\n",
    "                # Update the number of correct predictions\n",
    "                num_correct += 1\n",
    "        # Keep track of the total number of examples (that have valid labels)\n",
    "        total += 1 \n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    accuracy = num_correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9df961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.9253\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbce4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong output values for tag_counts dictionary.\n",
      "\t Expected: 0.8888563993099213.\n",
      "\t Got: 0.9252731866191825.\n",
      "Wrong output values for tag_counts dictionary.\n",
      "\t Expected: 0.876.\n",
      "\t Got: 0.9125.\n",
      "\u001b[92m 0  Tests passed\n",
      "\u001b[91m 2  Tests failed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_predict_pos(predict_pos, prep, y, emission_counts, vocab, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd35ee",
   "metadata": {},
   "source": [
    "## Exercise 3 - create_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6738895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 GRADED FUNCTION: create_transition_matrix\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    # Get a sorted list of unique POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Count the number of unique POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # Get the unique transition tuples (previous POS, current POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Go through each row of the transition matrix A\n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        # Go through each column of the transition matrix A\n",
    "        for j in range(num_tags):\n",
    "\n",
    "            # Initialize the count of the (prev POS, current POS) to zero\n",
    "            count = 0\n",
    "        \n",
    "            # Define the tuple (prev POS, current POS)\n",
    "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
    "            key = (all_tags[i], all_tags[j]) # tuple of form (tag,tag)\n",
    "\n",
    "            # Check if the (prev POS, current POS) tuple \n",
    "            # exists in the transition counts dictionary\n",
    "            if key in trans_keys: # Replace None in this line with the proper condition.\n",
    "                \n",
    "                # Get count from the transition_counts dictionary \n",
    "                # for the (prev POS, current POS) tuple\n",
    "                count = transition_counts[key]                \n",
    "\n",
    "            # Get the count of the previous tag (index position i) from tag_counts\n",
    "            count_prev_tag = tag_counts[key[0]]\n",
    "            \n",
    "            # Apply smoothing using count of the tuple, alpha, \n",
    "            # count of previous tag, alpha, and total number of tags\n",
    "            A[i,j] = (count + alpha) / ( count_prev_tag + (alpha * num_tags))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c253100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.000007040\n",
      "A at row 3, col 1: 0.1691\n",
      "View a subset of transition matrix A\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44d0ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_create_transition_matrix(create_transition_matrix, tag_counts, transition_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892b185",
   "metadata": {},
   "source": [
    "## Exercise 4 - create_emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1db49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 GRADED FUNCTION: create_emission_matrix\n",
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index.\n",
    "               within the function it'll be treated as a list\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    \n",
    "    # get the number of POS tag\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Get a list of all POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Get the total number of unique words in the vocabulary\n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    # Initialize the emission matrix B with places for\n",
    "    # tags in the rows and words in the columns\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    # Get a set of all (POS, word) tuples \n",
    "    # from the keys of the emission_counts dictionary\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # Go through each row (POS tags)\n",
    "    for i in range(num_tags): # Replace None in this line with the proper range.\n",
    "        \n",
    "        # Go through each column (words)\n",
    "        for j in range(num_words): # Replace None in this line with the proper range.\n",
    "\n",
    "            # Initialize the emission count for the (POS tag, word) to zero\n",
    "            count = 0 \n",
    "                    \n",
    "            # Define the (POS tag, word) tuple for this row and column\n",
    "            key = (all_tags[i], vocab[j]) # tuple of form (tag,word)\n",
    "\n",
    "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
    "            if key in emis_keys: # Replace None in this line with the proper condition.\n",
    "        \n",
    "                # Get the count of (POS tag, word) from the emission_counts d\n",
    "                count = emission_counts[key]\n",
    "                \n",
    "            # Get the count of the POS tag\n",
    "            count_tag = tag_counts[key[0]]\n",
    "                \n",
    "            # Apply smoothing and store the smoothed value \n",
    "            # into the emission matrix B for this row and column\n",
    "            B[i,j] = (count + alpha) / (count_tag + (num_words * alpha))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84871ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "alpha = 0.001\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f69703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_create_emission_matrix(create_emission_matrix, tag_counts, emission_counts, list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d40e2f",
   "metadata": {},
   "source": [
    "## Exercise 5 - initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b7466a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 GRADED FUNCTION: initialize\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    # Get the total number of unique POS tags\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix \n",
    "    # POS tags in the rows, number of words in the corpus as the columns\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    # Initialize best_paths matrix\n",
    "    # POS tags in the rows, number of words in the corpus as columns\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    # Define the start token\n",
    "    s_idx = states.index(\"--s--\")\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # Go through each of the POS tags\n",
    "    for i in range(num_tags): # Replace None in this line with the proper range.\n",
    "        \n",
    "        # Initialize best_probs at POS tag 'i', column 0\n",
    "        # Check the formula in the instructions above\n",
    "        best_probs[i,0] = math.log(A[s_idx, i]) + math.log(B[i, vocab[corpus[0]]])\n",
    "            \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a54d941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0948a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,3]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\")\n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a1b8cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_initialize(initialize, states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e2533",
   "metadata": {},
   "source": [
    "## Exercise 6 - viterbi_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3208df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 GRADED FUNCTION: viterbi_forward\n",
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab, verbose=True):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transition and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Go through every word in the corpus starting from word 1\n",
    "    # Recall that word 0 was initialized in `initialize()`\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        \n",
    "        # Print number of words processed, every 5000 words\n",
    "        if i % 5000 == 0 and verbose:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        ### START CODE HERE  (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None')  ###\n",
    "        \n",
    "        # For each unique POS tag that the current word can be\n",
    "        for j in range(num_tags): # Replace None in this line with the proper range. # for every pos tag\n",
    "            \n",
    "            # Initialize best_prob for word i to negative infinity\n",
    "            best_prob_i = float(\"-inf\")\n",
    "            \n",
    "            # Initialize best_path for current word i to None\n",
    "            best_path_i = None # Do not replace this None # @KEEPTHIS\n",
    "\n",
    "            # For each POS tag that the previous word can be:\n",
    "            for k in range(num_tags): # Replace None in this line with the proper range.\n",
    "            \n",
    "                # Calculate the probability = None\n",
    "                # best probs of POS tag k, previous word i-1 + \n",
    "                # log(prob of transition from POS k to POS j) + \n",
    "                # log(prob that emission of POS j is word i)\n",
    "                prob = best_probs[k, i-1] + math.log(A[k,j]) + math.log(B[j,vocab[test_corpus[i]]])\n",
    "\n",
    "                # check if this path's probability is greater than\n",
    "                # the best probability up to and before this point\n",
    "                if prob > best_prob_i: # Replace None in this line with the proper condition.\n",
    "                    \n",
    "                    # Keep track of the best probability\n",
    "                    best_prob_i = prob\n",
    "                    \n",
    "                    # keep track of the POS tag of the previous word\n",
    "                    # that is part of the best path.  \n",
    "                    # Save the index (integer) associated with \n",
    "                    # that previous word's POS tag\n",
    "                    best_path_i = k\n",
    "\n",
    "            # Save the best probability for the \n",
    "            # given current word's POS tag\n",
    "            # and the position of the current word inside the corpus\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            \n",
    "            # Save the unique integer ID of the previous POS tag\n",
    "            # into best_paths matrix, for the POS tag of the current word\n",
    "            # and the position of the current word inside the corpus.\n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f628db48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd8c379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -24.7822\n",
      "best_probs[0,4]: -49.5601\n"
     ]
    }
   ],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10fe9b83",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './support_files/best_probs_initilized.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test your function: this test may take some time to run\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mw2_unittest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_viterbi_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviterbi_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hieu\\OneDrive\\MSE - FPT\\Fall 2025\\NLP501\\Assignment\\Ex2\\w2_unittest.py:940\u001b[39m, in \u001b[36mtest_viterbi_forward\u001b[39m\u001b[34m(target, A, B, test_corpus, vocab)\u001b[39m\n\u001b[32m    929\u001b[39m successful_cases = \u001b[32m0\u001b[39m\n\u001b[32m    930\u001b[39m failed_cases = []\n\u001b[32m    932\u001b[39m test_cases = [\n\u001b[32m    933\u001b[39m     {\n\u001b[32m    934\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdefault_check\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    935\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    936\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m: A,\n\u001b[32m    937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m: B,\n\u001b[32m    938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtest_corpus\u001b[39m\u001b[33m\"\u001b[39m: test_corpus,\n\u001b[32m    939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_probs\u001b[39m\u001b[33m\"\u001b[39m: pickle.load(\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m                 \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./support_files/best_probs_initilized.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m             ),\n\u001b[32m    942\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_paths\u001b[39m\u001b[33m\"\u001b[39m: pickle.load(\n\u001b[32m    943\u001b[39m                 \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m./support_files/best_paths_initilized.pkl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    944\u001b[39m             ),\n\u001b[32m    945\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mvocab\u001b[39m\u001b[33m\"\u001b[39m: vocab,\n\u001b[32m    946\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    947\u001b[39m         },\n\u001b[32m    948\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexpected\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    949\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_probs0:5\u001b[39m\u001b[33m\"\u001b[39m: np.array(\n\u001b[32m    950\u001b[39m                 [\n\u001b[32m    951\u001b[39m                     [\n\u001b[32m    952\u001b[39m                         -\u001b[32m22.60982633\u001b[39m,\n\u001b[32m    953\u001b[39m                         -\u001b[32m24.78215633\u001b[39m,\n\u001b[32m    954\u001b[39m                         -\u001b[32m34.08246498\u001b[39m,\n\u001b[32m    955\u001b[39m                         -\u001b[32m34.34107105\u001b[39m,\n\u001b[32m    956\u001b[39m                         -\u001b[32m49.56012613\u001b[39m,\n\u001b[32m    957\u001b[39m                     ],\n\u001b[32m    958\u001b[39m                     [\n\u001b[32m    959\u001b[39m                         -\u001b[32m23.07660654\u001b[39m,\n\u001b[32m    960\u001b[39m                         -\u001b[32m24.51583896\u001b[39m,\n\u001b[32m    961\u001b[39m                         -\u001b[32m35.04774303\u001b[39m,\n\u001b[32m    962\u001b[39m                         -\u001b[32m35.28281026\u001b[39m,\n\u001b[32m    963\u001b[39m                         -\u001b[32m50.52540418\u001b[39m,\n\u001b[32m    964\u001b[39m                     ],\n\u001b[32m    965\u001b[39m                     [\n\u001b[32m    966\u001b[39m                         -\u001b[32m23.57298822\u001b[39m,\n\u001b[32m    967\u001b[39m                         -\u001b[32m29.98305064\u001b[39m,\n\u001b[32m    968\u001b[39m                         -\u001b[32m31.98004656\u001b[39m,\n\u001b[32m    969\u001b[39m                         -\u001b[32m38.99187549\u001b[39m,\n\u001b[32m    970\u001b[39m                         -\u001b[32m47.45770771\u001b[39m,\n\u001b[32m    971\u001b[39m                     ],\n\u001b[32m    972\u001b[39m                     [\n\u001b[32m    973\u001b[39m                         -\u001b[32m19.76726066\u001b[39m,\n\u001b[32m    974\u001b[39m                         -\u001b[32m25.7122143\u001b[39m,\n\u001b[32m    975\u001b[39m                         -\u001b[32m31.54577612\u001b[39m,\n\u001b[32m    976\u001b[39m                         -\u001b[32m37.38331695\u001b[39m,\n\u001b[32m    977\u001b[39m                         -\u001b[32m47.02343727\u001b[39m,\n\u001b[32m    978\u001b[39m                     ],\n\u001b[32m    979\u001b[39m                     [\n\u001b[32m    980\u001b[39m                         -\u001b[32m24.74325104\u001b[39m,\n\u001b[32m    981\u001b[39m                         -\u001b[32m28.78696025\u001b[39m,\n\u001b[32m    982\u001b[39m                         -\u001b[32m31.458494\u001b[39m,\n\u001b[32m    983\u001b[39m                         -\u001b[32m36.00456711\u001b[39m,\n\u001b[32m    984\u001b[39m                         -\u001b[32m46.93615515\u001b[39m,\n\u001b[32m    985\u001b[39m                     ],\n\u001b[32m    986\u001b[39m                 ]\n\u001b[32m    987\u001b[39m             ),\n\u001b[32m    988\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_probs30:35\u001b[39m\u001b[33m\"\u001b[39m: np.array(\n\u001b[32m    989\u001b[39m                 [\n\u001b[32m    990\u001b[39m                     [\n\u001b[32m    991\u001b[39m                         -\u001b[32m202.75618827\u001b[39m,\n\u001b[32m    992\u001b[39m                         -\u001b[32m208.38838519\u001b[39m,\n\u001b[32m    993\u001b[39m                         -\u001b[32m210.46938402\u001b[39m,\n\u001b[32m    994\u001b[39m                         -\u001b[32m210.15943098\u001b[39m,\n\u001b[32m    995\u001b[39m                         -\u001b[32m223.79223672\u001b[39m,\n\u001b[32m    996\u001b[39m                     ],\n\u001b[32m    997\u001b[39m                     [\n\u001b[32m    998\u001b[39m                         -\u001b[32m202.58297597\u001b[39m,\n\u001b[32m    999\u001b[39m                         -\u001b[32m217.72266765\u001b[39m,\n\u001b[32m   1000\u001b[39m                         -\u001b[32m207.23725672\u001b[39m,\n\u001b[32m   1001\u001b[39m                         -\u001b[32m215.529735\u001b[39m,\n\u001b[32m   1002\u001b[39m                         -\u001b[32m224.13957203\u001b[39m,\n\u001b[32m   1003\u001b[39m                     ],\n\u001b[32m   1004\u001b[39m                     [\n\u001b[32m   1005\u001b[39m                         -\u001b[32m202.00878092\u001b[39m,\n\u001b[32m   1006\u001b[39m                         -\u001b[32m214.23093833\u001b[39m,\n\u001b[32m   1007\u001b[39m                         -\u001b[32m217.41021623\u001b[39m,\n\u001b[32m   1008\u001b[39m                         -\u001b[32m220.73768708\u001b[39m,\n\u001b[32m   1009\u001b[39m                         -\u001b[32m222.03338753\u001b[39m,\n\u001b[32m   1010\u001b[39m                     ],\n\u001b[32m   1011\u001b[39m                     [\n\u001b[32m   1012\u001b[39m                         -\u001b[32m200.44016117\u001b[39m,\n\u001b[32m   1013\u001b[39m                         -\u001b[32m209.46937757\u001b[39m,\n\u001b[32m   1014\u001b[39m                         -\u001b[32m209.06951664\u001b[39m,\n\u001b[32m   1015\u001b[39m                         -\u001b[32m216.22297765\u001b[39m,\n\u001b[32m   1016\u001b[39m                         -\u001b[32m221.09669653\u001b[39m,\n\u001b[32m   1017\u001b[39m                     ],\n\u001b[32m   1018\u001b[39m                     [\n\u001b[32m   1019\u001b[39m                         -\u001b[32m208.74189499\u001b[39m,\n\u001b[32m   1020\u001b[39m                         -\u001b[32m214.62088817\u001b[39m,\n\u001b[32m   1021\u001b[39m                         -\u001b[32m209.79346523\u001b[39m,\n\u001b[32m   1022\u001b[39m                         -\u001b[32m213.52623459\u001b[39m,\n\u001b[32m   1023\u001b[39m                         -\u001b[32m228.70417526\u001b[39m,\n\u001b[32m   1024\u001b[39m                     ],\n\u001b[32m   1025\u001b[39m                 ]\n\u001b[32m   1026\u001b[39m             ),\n\u001b[32m   1027\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_paths0:5\u001b[39m\u001b[33m\"\u001b[39m: np.array(\n\u001b[32m   1028\u001b[39m                 [\n\u001b[32m   1029\u001b[39m                     [\u001b[32m0\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m   1030\u001b[39m                     [\u001b[32m0\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m   1031\u001b[39m                     [\u001b[32m0\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m   1032\u001b[39m                     [\u001b[32m0\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m   1033\u001b[39m                     [\u001b[32m0\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m   1034\u001b[39m                 ]\n\u001b[32m   1035\u001b[39m             ),\n\u001b[32m   1036\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_paths30:35\u001b[39m\u001b[33m\"\u001b[39m: np.array(\n\u001b[32m   1037\u001b[39m                 [\n\u001b[32m   1038\u001b[39m                     [\u001b[32m20\u001b[39m, \u001b[32m19\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m21\u001b[39m],\n\u001b[32m   1039\u001b[39m                     [\u001b[32m20\u001b[39m, \u001b[32m19\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m21\u001b[39m],\n\u001b[32m   1040\u001b[39m                     [\u001b[32m20\u001b[39m, \u001b[32m19\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m21\u001b[39m],\n\u001b[32m   1041\u001b[39m                     [\u001b[32m20\u001b[39m, \u001b[32m19\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m21\u001b[39m],\n\u001b[32m   1042\u001b[39m                     [\u001b[32m35\u001b[39m, \u001b[32m19\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m11\u001b[39m, \u001b[32m34\u001b[39m],\n\u001b[32m   1043\u001b[39m                 ]\n\u001b[32m   1044\u001b[39m             ),\n\u001b[32m   1045\u001b[39m         },\n\u001b[32m   1046\u001b[39m     }\n\u001b[32m   1047\u001b[39m ]\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n\u001b[32m   1050\u001b[39m     result_best_probs, result_best_paths = target(**test_case[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './support_files/best_probs_initilized.pkl'"
     ]
    }
   ],
   "source": [
    "# Test your function: this test may take some time to run\n",
    "w2_unittest.test_viterbi_forward(viterbi_forward, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245724fa",
   "metadata": {},
   "source": [
    "## Exercise 7 - viterbi_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c05eb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 GRADED FUNCTION: viterbi_backward\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m # DO NOT replace the \"None\"\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m # DO NOT replace the \"None\"\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    ## Step 1 ##\n",
    "    \n",
    "    # Go through each POS tag for the last word (last column of best_probs)\n",
    "    # in order to find the row (POS tag integer ID) \n",
    "    # with highest probability for the last word\n",
    "    for k in range(num_tags): # Replace None in this line with the proper range.\n",
    "\n",
    "        # If the probability of POS tag at row k \n",
    "        # is better than the previously best probability for the last word:\n",
    "        if best_probs[k, -1] > best_prob_for_last_word: # Replace None in this line with the proper condition.\n",
    "            \n",
    "            # Store the new best probability for the last word\n",
    "            best_prob_for_last_word = best_probs[k, -1]\n",
    "\n",
    "            # Store the unique integer ID of the POS tag\n",
    "            # which is also the row number in best_probs\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    # Convert the last word's predicted POS tag\n",
    "    # from its unique integer ID into the string representation\n",
    "    # using the 'states' list\n",
    "    # store this in the 'pred' array for the last word\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "    \n",
    "    ## Step 2 ##\n",
    "    # Find the best POS tags by walking backward through the best_paths\n",
    "    # From the last word in the corpus to the 0th word in the corpus\n",
    "    for i in range(m-1, 0, -1): # Replace None in this line with the proper range.\n",
    "        # Retrieve the unique integer ID of\n",
    "        # the POS tag for the word at position 'i' in the corpus\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        \n",
    "        # In best_paths, go to the row representing the POS tag of word i\n",
    "        # and the column representing the word's position in the corpus\n",
    "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        \n",
    "        # Get the previous word's POS tag in string form\n",
    "        # Use the 'states' list, \n",
    "        # where the key is the unique integer ID of the POS tag,\n",
    "        # and the value is the string representation of that POS tag\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "        \n",
    "     ### END CODE HERE ###\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25b23761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ce8dcc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './support_files/best_probs_trained.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test your function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mw2_unittest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_viterbi_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviterbi_backward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hieu\\OneDrive\\MSE - FPT\\Fall 2025\\NLP501\\Assignment\\Ex2\\w2_unittest.py:1171\u001b[39m, in \u001b[36mtest_viterbi_backward\u001b[39m\u001b[34m(target, corpus, states)\u001b[39m\n\u001b[32m   1162\u001b[39m successful_cases = \u001b[32m0\u001b[39m\n\u001b[32m   1163\u001b[39m failed_cases = []\n\u001b[32m   1165\u001b[39m test_cases = [\n\u001b[32m   1166\u001b[39m     {\n\u001b[32m   1167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdefault_check\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1168\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   1169\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcorpus\u001b[39m\u001b[33m\"\u001b[39m: corpus,\n\u001b[32m   1170\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_probs\u001b[39m\u001b[33m\"\u001b[39m: pickle.load(\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m                 \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./support_files/best_probs_trained.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m             ),\n\u001b[32m   1173\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbest_paths\u001b[39m\u001b[33m\"\u001b[39m: pickle.load(\n\u001b[32m   1174\u001b[39m                 \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m./support_files/best_paths_trained.pkl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1175\u001b[39m             ),\n\u001b[32m   1176\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstates\u001b[39m\u001b[33m\"\u001b[39m: states,\n\u001b[32m   1177\u001b[39m         },\n\u001b[32m   1178\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexpected\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   1179\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpred_len\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m34199\u001b[39m,\n\u001b[32m   1180\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpred_head\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mDT\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPOS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1185\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mMD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1186\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mVB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1187\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mVBN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1188\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mIN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1189\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mJJ\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1190\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1191\u001b[39m             ],\n\u001b[32m   1192\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpred_tail\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1193\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPRP\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1194\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mMD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1195\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mRB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1196\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mVB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1197\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPRP\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1198\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mRB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1199\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mIN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1200\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPRP\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1201\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1202\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m--s--\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1203\u001b[39m             ],\n\u001b[32m   1204\u001b[39m         },\n\u001b[32m   1205\u001b[39m     }\n\u001b[32m   1206\u001b[39m ]\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n\u001b[32m   1209\u001b[39m     result = target(**test_case[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './support_files/best_probs_trained.pkl'"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_viterbi_backward(viterbi_backward, prep, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80625d10",
   "metadata": {},
   "source": [
    "## Exercise 8 - compute_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51c28209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Zip together the prediction and the labels\n",
    "    for prediction, y in zip(pred, y):\n",
    "        \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        # Split the label into the word and the POS tag\n",
    "        word_tag_tuple = y.split()\n",
    "        \n",
    "        # Check that there is actually a word and a tag\n",
    "        # no more and no less than 2 items\n",
    "        if len(word_tag_tuple) != 2: # Replace None in this line with the proper condition.\n",
    "            continue\n",
    "\n",
    "        # store the word and tag separately\n",
    "        word, tag = word_tag_tuple[0], word_tag_tuple[1]\n",
    "        \n",
    "        # Check if the POS tag label matches the prediction\n",
    "        if prediction == tag: # Replace None in this line with the proper condition.\n",
    "            \n",
    "            # count the number of times that the prediction\n",
    "            # and label match\n",
    "            num_correct += 1\n",
    "            \n",
    "        # keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "518d930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9531\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1248b6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w2_unittest.test_compute_accuracy(compute_accuracy, pred, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
